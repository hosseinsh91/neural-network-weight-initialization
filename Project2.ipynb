{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b61f9d42-a844-4121-9a8e-fa43dee4913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Flatten, Dense\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c165dab-d67e-4634-a29f-f86b8d090dd5",
   "metadata": {},
   "source": [
    "#### In the first level I defined a neural network model for the given task and I set 2 hidden layers with 50 neurons for each one, and I set the weights of these hidden layers using a kernel initializer with zero values then printed the weights of hidden layers.\n",
    "#### I fitted the model and monitored the accuracy at each epoch and the accuracy remained unchanged during the training, showing no improvement over the epochs. After completion of training, I printed the weights of the hidden layers once again and  found that they were still set to zero.\n",
    "\n",
    "#### The reason of this lack of weight changes in the hidden layers:  by setting wights of layers to zero in neural network , the gradient updates during the learning process become zero as well  and as the result weights donâ€™t be updated after each epoch and accuracy and weights remains the same . In cases where there might be any changes in the values after training , they are very small and very close to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "819d6856-d985-4b4c-b51a-a55c966c1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d8db2ab5-408a-4bc1-993e-3dddffee8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train/255.0, X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5534216d-6a3d-46c1-99fb-3ae4a5c0aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(Dense(50, activation='selu', kernel_initializer='zeros'))\n",
    "model.add(Dense(50, activation='selu', kernel_initializer='zeros'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "82370a0e-0735-4b45-925c-d6538d401b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_22 (Flatten)        (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 50)                153650    \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 156,710\n",
      "Trainable params: 156,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d6102ba3-f705-4979-af20-2de5285ce38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer='sgd',\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "59e871a0-0bf9-441b-bb16-471fe0278927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights:\n",
      "weights in layers 1 : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "weights in layers 2 : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "weights in layers 3 : \n",
      " [[ 0.10973138 -0.29080155  0.20979276  0.15508899  0.26701483 -0.01455259\n",
      "  -0.07518053  0.07037285  0.08500731  0.23889253]\n",
      " [ 0.22059211  0.2220895   0.28648213 -0.29442093 -0.17442165 -0.2256102\n",
      "   0.21362266  0.07035491 -0.28971678  0.310693  ]\n",
      " [-0.21696228 -0.17848392 -0.13951442 -0.06944451 -0.14128174 -0.0068965\n",
      "   0.19599482  0.13682503 -0.064973    0.14033887]\n",
      " [ 0.03322631 -0.03104779  0.25720468 -0.25893655 -0.17637068 -0.29059225\n",
      "  -0.12933713  0.08127904  0.24131796  0.26122352]\n",
      " [-0.2840137   0.27168414 -0.04420233 -0.29106942  0.2912096   0.29333863\n",
      "   0.25912932  0.25949213  0.23420104  0.21361408]\n",
      " [-0.24103072  0.15349144 -0.05944479 -0.22850399 -0.19773698 -0.01263967\n",
      "  -0.12551862 -0.26824778  0.31460676 -0.16162132]\n",
      " [-0.1705607  -0.03518122 -0.2603634   0.202952    0.1396634  -0.2603285\n",
      "   0.18292141 -0.26945138 -0.30735487  0.15242988]\n",
      " [-0.15034975 -0.2202887  -0.12344143  0.06203827  0.27689043 -0.29988953\n",
      "   0.00769153 -0.1176573   0.20587996  0.09130117]\n",
      " [-0.03597814  0.2759188   0.01480681  0.00659657 -0.22655964 -0.21849921\n",
      "   0.22602054 -0.29449484  0.22028449 -0.2771874 ]\n",
      " [-0.05588073  0.06726313  0.23256466  0.1060442   0.07560116 -0.1449674\n",
      "   0.06387648  0.01252735 -0.0409658   0.04379076]\n",
      " [ 0.04516083 -0.06550184 -0.28729305 -0.30067816  0.20972428  0.26510265\n",
      "  -0.10601917 -0.09812823  0.04578328  0.22199568]\n",
      " [ 0.19223651  0.24120775  0.18784806 -0.07363155  0.16553617 -0.1980479\n",
      "   0.02325094 -0.05438927  0.03924301  0.03223261]\n",
      " [ 0.2938862  -0.06867768  0.20530054  0.2140005  -0.11585748  0.1455321\n",
      "  -0.1494411  -0.20633626 -0.18290392  0.05942714]\n",
      " [-0.30037922  0.08655718  0.29719517  0.20899841 -0.11774175  0.20656344\n",
      "   0.20912674  0.19173923 -0.22077566  0.12330359]\n",
      " [ 0.1143392  -0.13230127 -0.09861973 -0.05530101 -0.04456973  0.13471419\n",
      "   0.19646588 -0.31273887  0.08210513 -0.15903807]\n",
      " [-0.00151718 -0.11136413 -0.28780347  0.13891834  0.08744314  0.26109943\n",
      "  -0.04909703  0.09324086  0.17816085 -0.15054208]\n",
      " [-0.13117419  0.22236255  0.16203305 -0.1986129   0.04334351 -0.01241094\n",
      "  -0.01881525 -0.11824296  0.14020556  0.18550488]\n",
      " [-0.23840028  0.10699379  0.08718416 -0.10434647 -0.079638   -0.18281314\n",
      "  -0.23873097 -0.04088649 -0.25917912  0.19074908]\n",
      " [-0.19095539  0.09593913  0.10112998 -0.13685314 -0.17308958  0.19277939\n",
      "   0.26515397  0.29513046  0.06981418 -0.21904665]\n",
      " [-0.06078297  0.08629927  0.1290473  -0.10260478  0.24460384  0.15113905\n",
      "  -0.08477615 -0.0171271  -0.11913843 -0.20397037]\n",
      " [-0.26038128 -0.04094821  0.00211301 -0.29513    -0.21340728 -0.22231455\n",
      "   0.02407777  0.27685967 -0.10366528  0.30408832]\n",
      " [ 0.18604091  0.06353003  0.09274754  0.24128154 -0.25522187 -0.2971258\n",
      "  -0.13585356  0.06563112  0.2552857   0.27617446]\n",
      " [ 0.09803376  0.00043601  0.29679647  0.17685682 -0.107951    0.15585092\n",
      "  -0.14737943  0.01806575 -0.1388606  -0.17389321]\n",
      " [ 0.11466408  0.04563135 -0.21098298 -0.06067392  0.13191834 -0.16013733\n",
      "   0.05160102 -0.14501573 -0.16572806  0.12545055]\n",
      " [-0.21982434 -0.16438943 -0.18245985 -0.27242193 -0.03970489 -0.04206595\n",
      "  -0.11777462 -0.27704114 -0.27820107  0.01348922]\n",
      " [ 0.0961642  -0.06057855  0.28101298 -0.03749961 -0.28914323 -0.07408746\n",
      "   0.20878085 -0.03825054  0.25496623  0.2526625 ]\n",
      " [ 0.02936718 -0.22325517  0.05622137  0.1783176  -0.03607252  0.1629822\n",
      "   0.20469531  0.11947018 -0.09283017 -0.17486143]\n",
      " [-0.0150609  -0.29793033  0.01154968 -0.12231676  0.22147879  0.13757429\n",
      "  -0.00886384  0.0367088  -0.1654751  -0.22141284]\n",
      " [-0.11337791  0.2565057  -0.22017282  0.08234641 -0.21101923  0.27693883\n",
      "  -0.02557075  0.19995615 -0.06054336  0.10517499]\n",
      " [ 0.0193277  -0.22530252 -0.25435558  0.15690485  0.2329404  -0.31559393\n",
      "   0.10046327 -0.1868172   0.1115008   0.05168721]\n",
      " [-0.29431146 -0.19616938  0.03073949 -0.18356295  0.21170244  0.2820529\n",
      "  -0.30696154  0.03140077 -0.20489252 -0.3135499 ]\n",
      " [ 0.26805392  0.21748427 -0.28900558 -0.01875493 -0.21039188 -0.03675923\n",
      "  -0.05388856  0.2283974  -0.21649785  0.05533653]\n",
      " [-0.05802217 -0.00681484  0.1484636  -0.24224329  0.0883666   0.19910172\n",
      "  -0.20682693 -0.18150023  0.11057392 -0.03116351]\n",
      " [-0.11042553  0.01519969  0.05829433 -0.1602112   0.12413928  0.13060617\n",
      "  -0.04438818  0.263028    0.04725748  0.16988584]\n",
      " [ 0.08886525 -0.1790769  -0.20224383  0.27532396 -0.20138554  0.22034004\n",
      "  -0.3012256  -0.1177091  -0.0524523   0.21957067]\n",
      " [-0.18344036  0.12020051 -0.16511841 -0.2954432   0.24760601 -0.29035643\n",
      "   0.21412024 -0.2959185  -0.25953504  0.02459666]\n",
      " [ 0.01559943  0.3018693   0.08498716 -0.09015472 -0.27396587 -0.02041361\n",
      "  -0.21327075  0.08816957  0.23757562  0.08204755]\n",
      " [-0.05767459 -0.15582226  0.22403345  0.23704079  0.1127522  -0.10492708\n",
      "  -0.01045302 -0.13456002  0.21064588 -0.29806024]\n",
      " [-0.22885299  0.10045475 -0.30381262  0.23416868 -0.19407001  0.16975352\n",
      "  -0.06399786 -0.19909264  0.20159844  0.0563983 ]\n",
      " [ 0.24041161 -0.14946643 -0.20995452 -0.29214388 -0.19165558 -0.24252564\n",
      "   0.01354727  0.10819355 -0.01631442  0.07118672]\n",
      " [ 0.3062198  -0.09079784  0.03662917 -0.23312786  0.10181481 -0.16052862\n",
      "  -0.00288799 -0.17652674  0.16670915 -0.19365618]\n",
      " [-0.09894429  0.04618755  0.10251588 -0.07860819  0.04630658 -0.30470106\n",
      "   0.03957197 -0.28397283  0.08212444 -0.26095983]\n",
      " [ 0.24880031 -0.05634803 -0.00872767 -0.03469026 -0.29169828 -0.30645353\n",
      "   0.16463023  0.23874196 -0.04185694  0.12034926]\n",
      " [ 0.08280268 -0.20761487  0.21607688 -0.2754635  -0.09444293 -0.07773089\n",
      "   0.16026181  0.10676324  0.2747617  -0.18917705]\n",
      " [ 0.1491242   0.16911417 -0.04553771 -0.31359613  0.12508246 -0.30323404\n",
      "   0.0633215   0.05591744 -0.0349012   0.09986231]\n",
      " [-0.30097598  0.01374397  0.14427769 -0.19460696  0.29838064  0.1170696\n",
      "   0.2937527  -0.3022193   0.10871702  0.17542002]\n",
      " [-0.24788703  0.2443982  -0.23617268  0.09784701  0.2736201  -0.2221924\n",
      "  -0.26091975  0.01819217 -0.19080143 -0.20470487]\n",
      " [ 0.23728624  0.00329074  0.02060932 -0.03422627 -0.11567669 -0.2589469\n",
      "   0.14700252 -0.02820209 -0.01849526 -0.11060633]\n",
      " [ 0.23529664  0.23892286 -0.10626797  0.05853385  0.18320271 -0.00888428\n",
      "  -0.06676076 -0.10092047 -0.09468947 -0.0372079 ]\n",
      " [ 0.19443539  0.30597833  0.05745947  0.13289559 -0.08072911 -0.02549008\n",
      "  -0.12568335  0.07004577  0.03266907  0.3090109 ]]\n"
     ]
    }
   ],
   "source": [
    "# Display the initial weights\n",
    "print(\"Initial Weights:\")\n",
    "for i in range(1, len(model.layers)):\n",
    "    weights = model.layers[i].get_weights()[0]\n",
    "    print('weights in layers %d :'%i , '\\n',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c66bb2a0-68cf-4c75-8304-47a13ce34e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0994 - val_loss: 2.3030 - val_accuracy: 0.0975\n",
      "Epoch 2/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0975 - val_loss: 2.3029 - val_accuracy: 0.0975\n",
      "Epoch 3/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0995 - val_loss: 2.3025 - val_accuracy: 0.1029\n",
      "Epoch 4/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3027 - accuracy: 0.1005 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0985 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0990 - val_loss: 2.3032 - val_accuracy: 0.0984\n",
      "Epoch 7/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0996 - val_loss: 2.3027 - val_accuracy: 0.0953\n",
      "Epoch 8/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0996 - val_loss: 2.3027 - val_accuracy: 0.0975\n",
      "Epoch 9/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0982 - val_loss: 2.3031 - val_accuracy: 0.0999\n",
      "Epoch 10/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0983 - val_loss: 2.3030 - val_accuracy: 0.0972\n",
      "Epoch 11/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0977 - val_loss: 2.3029 - val_accuracy: 0.0953\n",
      "Epoch 12/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0973 - val_loss: 2.3028 - val_accuracy: 0.0975\n",
      "Epoch 13/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0992 - val_loss: 2.3027 - val_accuracy: 0.0975\n",
      "Epoch 14/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0961 - val_loss: 2.3030 - val_accuracy: 0.0953\n",
      "Epoch 15/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0996 - val_loss: 2.3029 - val_accuracy: 0.1015\n",
      "Epoch 16/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0993 - val_loss: 2.3028 - val_accuracy: 0.1008\n",
      "Epoch 17/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0983 - val_loss: 2.3029 - val_accuracy: 0.0975\n",
      "Epoch 18/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0988 - val_loss: 2.3031 - val_accuracy: 0.0984\n",
      "Epoch 19/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0982 - val_loss: 2.3027 - val_accuracy: 0.0999\n",
      "Epoch 20/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0981 - val_loss: 2.3028 - val_accuracy: 0.0953\n",
      "Epoch 21/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0967 - val_loss: 2.3029 - val_accuracy: 0.0984\n",
      "Epoch 22/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0992 - val_loss: 2.3026 - val_accuracy: 0.0953\n",
      "Epoch 23/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0987 - val_loss: 2.3028 - val_accuracy: 0.0984\n",
      "Epoch 24/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0993 - val_loss: 2.3029 - val_accuracy: 0.0972\n",
      "Epoch 25/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0979 - val_loss: 2.3031 - val_accuracy: 0.0999\n",
      "Epoch 26/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0992 - val_loss: 2.3025 - val_accuracy: 0.1015\n",
      "Epoch 27/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0987 - val_loss: 2.3027 - val_accuracy: 0.0972\n",
      "Epoch 28/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0960 - val_loss: 2.3029 - val_accuracy: 0.0972\n",
      "Epoch 29/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.1004 - val_loss: 2.3027 - val_accuracy: 0.0953\n",
      "Epoch 30/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0996 - val_loss: 2.3025 - val_accuracy: 0.1015\n",
      "Epoch 31/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0993 - val_loss: 2.3030 - val_accuracy: 0.0953\n",
      "Epoch 32/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.1002 - val_loss: 2.3026 - val_accuracy: 0.1065\n",
      "Epoch 33/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0972 - val_loss: 2.3028 - val_accuracy: 0.0999\n",
      "Epoch 34/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0994 - val_loss: 2.3030 - val_accuracy: 0.0975\n",
      "Epoch 35/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0992 - val_loss: 2.3029 - val_accuracy: 0.0972\n",
      "Epoch 36/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.0972\n",
      "Epoch 37/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0998 - val_loss: 2.3028 - val_accuracy: 0.0953\n",
      "Epoch 38/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0968 - val_loss: 2.3029 - val_accuracy: 0.1008\n",
      "Epoch 39/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0988 - val_loss: 2.3029 - val_accuracy: 0.1015\n",
      "Epoch 40/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0969 - val_loss: 2.3029 - val_accuracy: 0.0953\n",
      "Epoch 41/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.1013 - val_loss: 2.3028 - val_accuracy: 0.0972\n",
      "Epoch 42/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0988 - val_loss: 2.3029 - val_accuracy: 0.1008\n",
      "Epoch 43/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0988 - val_loss: 2.3033 - val_accuracy: 0.0953\n",
      "Epoch 44/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.1002 - val_loss: 2.3031 - val_accuracy: 0.0953\n",
      "Epoch 45/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3028 - accuracy: 0.0974 - val_loss: 2.3025 - val_accuracy: 0.1065\n",
      "Epoch 46/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0971 - val_loss: 2.3027 - val_accuracy: 0.0972\n",
      "Epoch 47/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0979 - val_loss: 2.3030 - val_accuracy: 0.0953\n",
      "Epoch 48/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3028 - accuracy: 0.0990 - val_loss: 2.3026 - val_accuracy: 0.1015\n",
      "Epoch 49/50\n",
      "1329/1329 [==============================] - 2s 2ms/step - loss: 2.3029 - accuracy: 0.0973 - val_loss: 2.3027 - val_accuracy: 0.1008\n",
      "Epoch 50/50\n",
      "1329/1329 [==============================] - 3s 2ms/step - loss: 2.3029 - accuracy: 0.0991 - val_loss: 2.3026 - val_accuracy: 0.0999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0e785fe50>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50 , validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ce65df3d-f6c7-40b7-956c-2c6c03114ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights after fitting:\n",
      "weights in layers 1 : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "weights in layers 2 : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "weights in layers 3 : \n",
      " [[ 0.109305   -0.28671145  0.208579    0.15420647  0.26440787 -0.0134424\n",
      "  -0.07314669  0.07042203  0.08446556  0.23728739]\n",
      " [ 0.21802984  0.2204547   0.28364557 -0.2908506  -0.17204575 -0.2228315\n",
      "   0.21301152  0.06895004 -0.285429    0.30672243]\n",
      " [-0.21475409 -0.17670375 -0.13842742 -0.06847437 -0.14040926 -0.00732858\n",
      "   0.19375858  0.13469824 -0.06461022  0.13784909]\n",
      " [ 0.03202296 -0.03067698  0.2548189  -0.2559132  -0.17488205 -0.2869599\n",
      "  -0.12699452  0.07950331  0.23858891  0.25845844]\n",
      " [-0.27964464  0.26929346 -0.0425675  -0.2858489   0.28888088  0.29184982\n",
      "   0.25779486  0.2579403   0.23304175  0.21264762]\n",
      " [-0.23958766  0.15038942 -0.06002605 -0.22628321 -0.19640078 -0.01257145\n",
      "  -0.12544839 -0.26634267  0.31006995 -0.16044821]\n",
      " [-0.169173   -0.03507647 -0.2584989   0.20040329  0.13747856 -0.25808415\n",
      "   0.17978106 -0.2668679  -0.30441764  0.14917903]\n",
      " [-0.14924061 -0.21828893 -0.12232865  0.06160766  0.27322528 -0.29637855\n",
      "   0.00667728 -0.11659095  0.20314723  0.0903514 ]\n",
      " [-0.03595042  0.27259707  0.01340746  0.00674856 -0.22360829 -0.21584575\n",
      "   0.22225136 -0.29189745  0.21816464 -0.27495745]\n",
      " [-0.05492374  0.06695334  0.23031944  0.10526233  0.07535633 -0.14275694\n",
      "   0.06380691  0.01253374 -0.04002975  0.04333591]\n",
      " [ 0.04429274 -0.06527479 -0.2840575  -0.29700783  0.20663002  0.26205203\n",
      "  -0.10484414 -0.09673049  0.04483809  0.22025159]\n",
      " [ 0.19029856  0.23915277  0.18629798 -0.07247433  0.16455157 -0.19500776\n",
      "   0.02363257 -0.05347315  0.03989702  0.03261181]\n",
      " [ 0.2910838  -0.0673621   0.20319767  0.21158238 -0.11426491  0.14394997\n",
      "  -0.14718485 -0.20370269 -0.18102986  0.05865688]\n",
      " [-0.29567268  0.08644776  0.29445162  0.2077138  -0.11562236  0.20499888\n",
      "   0.20849839  0.19020835 -0.21779339  0.12135546]\n",
      " [ 0.11307603 -0.131011   -0.09873831 -0.05434566 -0.04417361  0.13292888\n",
      "   0.1933137  -0.30934668  0.0810845  -0.15773329]\n",
      " [-0.00089639 -0.11005414 -0.28448075  0.13778439  0.08634676  0.2582316\n",
      "  -0.04923608  0.09306751  0.17592938 -0.14815202]\n",
      " [-0.12990412  0.21982248  0.1604704  -0.19571228  0.0429625  -0.01120978\n",
      "  -0.01783065 -0.11700874  0.13886698  0.18373404]\n",
      " [-0.23691797  0.10479841  0.0859412  -0.1042342  -0.07990615 -0.18124765\n",
      "  -0.23587517 -0.04159545 -0.25748435  0.18745439]\n",
      " [-0.18799393  0.09514229  0.09978439 -0.13459495 -0.17067426  0.19094022\n",
      "   0.26265398  0.2918748   0.06977306 -0.21690781]\n",
      " [-0.05996385  0.08502151  0.12739325 -0.10162092  0.24198228  0.14967349\n",
      "  -0.08379093 -0.01668999 -0.11770216 -0.2016093 ]\n",
      " [-0.2584866  -0.04105717  0.00198054 -0.2921766  -0.21213329 -0.22037631\n",
      "   0.02442947  0.27249882 -0.10310744  0.2997196 ]\n",
      " [ 0.18408115  0.06380486  0.09288512  0.23910241 -0.25200543 -0.29310167\n",
      "  -0.13372938  0.06501534  0.25266004  0.27377388]\n",
      " [ 0.09748994  0.0007425   0.29362044  0.17478353 -0.10638395  0.15425469\n",
      "  -0.14518829  0.01808431 -0.13729693 -0.17214608]\n",
      " [ 0.11283495  0.04494055 -0.20906392 -0.06025758  0.13008946 -0.15875976\n",
      "   0.05061715 -0.14357795 -0.16393326  0.12383721]\n",
      " [-0.21914463 -0.16450359 -0.18227848 -0.27083433 -0.04126656 -0.04330022\n",
      "  -0.11781582 -0.27569252 -0.27690357  0.0113491 ]\n",
      " [ 0.09554648 -0.05880957  0.2782633  -0.03587528 -0.2853507  -0.07222261\n",
      "   0.20774452 -0.03799707  0.2527859   0.2499382 ]\n",
      " [ 0.03002946 -0.22026347  0.05541491  0.17674515 -0.03535088  0.16101672\n",
      "   0.20244752  0.11855209 -0.09135558 -0.17319897]\n",
      " [-0.01502246 -0.2952326   0.01061084 -0.1214156   0.21833585  0.13540164\n",
      "  -0.00926352  0.03610778 -0.16392969 -0.21933866]\n",
      " [-0.11150397  0.25397596 -0.21706736  0.08186363 -0.20855734  0.27392027\n",
      "  -0.02486525  0.1983927  -0.06000852  0.10408679]\n",
      " [ 0.01862257 -0.22303677 -0.2520008   0.15534277  0.22987969 -0.3122638\n",
      "   0.09795352 -0.18480301  0.11002177  0.05103628]\n",
      " [-0.2916988  -0.19556549  0.02930363 -0.18267451  0.20792037  0.27804276\n",
      "  -0.30440584  0.03047088 -0.20406146 -0.31088436]\n",
      " [ 0.2649201   0.21537624 -0.28533033 -0.01906344 -0.20822316 -0.03707246\n",
      "  -0.0533619   0.22589974 -0.21392776  0.05475914]\n",
      " [-0.05778195 -0.00739544  0.14647523 -0.23954743  0.08689574  0.19721971\n",
      "  -0.20436323 -0.17968506  0.1088004  -0.03068268]\n",
      " [-0.10885735  0.01531724  0.05853523 -0.15777771  0.1228202   0.12980804\n",
      "  -0.04276694  0.26055792  0.04701996  0.16873418]\n",
      " [ 0.0879372  -0.17711924 -0.1997481   0.2720116  -0.19992189  0.21738695\n",
      "  -0.2980796  -0.11620542 -0.05325176  0.216991  ]\n",
      " [-0.18265602  0.11773778 -0.16497074 -0.29268166  0.24420872 -0.2878547\n",
      "   0.21072996 -0.293733   -0.25700948  0.0229491 ]\n",
      " [ 0.01507044  0.29854742  0.08471969 -0.08880936 -0.27083635 -0.01948023\n",
      "  -0.2103122   0.08694274  0.2350025   0.08159948]\n",
      " [-0.05674012 -0.15418863  0.22101149  0.23456554  0.1118516  -0.103246\n",
      "  -0.01109094 -0.13303629  0.20841154 -0.29456323]\n",
      " [-0.22618474  0.09891409 -0.3008028   0.23193698 -0.19227387  0.16806722\n",
      "  -0.06415138 -0.19655742  0.19822636  0.05537052]\n",
      " [ 0.2365975  -0.14814055 -0.20816931 -0.28967437 -0.19062552 -0.24099213\n",
      "   0.0128135   0.10583385 -0.01635103  0.06998813]\n",
      " [ 0.30222872 -0.09007908  0.03552096 -0.23079832  0.10052985 -0.15888724\n",
      "  -0.00360386 -0.17504276  0.16517921 -0.19119431]\n",
      " [-0.09880657  0.04474045  0.09997825 -0.07827625  0.0455558  -0.30159155\n",
      "   0.03777637 -0.2818568   0.08094195 -0.2589504 ]\n",
      " [ 0.24586685 -0.05504886 -0.00843666 -0.03430879 -0.288479   -0.3035155\n",
      "   0.16314977  0.23553342 -0.0407776   0.11875869]\n",
      " [ 0.08176126 -0.20523535  0.21320677 -0.27198872 -0.09344629 -0.07665247\n",
      "   0.15854366  0.10495164  0.27225786 -0.18716809]\n",
      " [ 0.14679712  0.16718732 -0.04498317 -0.31031564  0.12356389 -0.29985726\n",
      "   0.06272797  0.05481624 -0.03378283  0.09899979]\n",
      " [-0.29732877  0.01343695  0.14216298 -0.1910175   0.2954369   0.11695567\n",
      "   0.2912494  -0.2986574   0.10797269  0.1733477 ]\n",
      " [-0.2459388   0.24058925 -0.23392853  0.09542905  0.26982155 -0.22033647\n",
      "  -0.25955644  0.01760837 -0.18953177 -0.20278323]\n",
      " [ 0.23440956  0.00343856  0.01991472 -0.03407553 -0.11427731 -0.25645226\n",
      "   0.14496402 -0.02841503 -0.01774841 -0.10972372]\n",
      " [ 0.2329562   0.23659527 -0.10478286  0.05786229  0.18168588 -0.00854626\n",
      "  -0.06619357 -0.09911948 -0.09310335 -0.03613153]\n",
      " [ 0.19283158  0.3036858   0.05840797  0.1321188  -0.07901621 -0.02413197\n",
      "  -0.12298455  0.07004528  0.03299215  0.30664185]]\n"
     ]
    }
   ],
   "source": [
    "# Display the initial weights\n",
    "print(\"Initial Weights after fitting:\")\n",
    "for i in range(1, len(model.layers)):\n",
    "    weights = model.layers[i].get_weights()[0]\n",
    "    print('weights in layers %d :'%i , '\\n',weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
